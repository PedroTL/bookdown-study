---
title: ""
author: ""
date: ""
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Multivariate Analysis

We have many variables with corresponding data, so we want to make sense of them.

### 1. Introduction - Background

-   Matrix Algebra
-   Summary Statistics
    -   Matrix Algebra
    -   Multivariate Normal Distribution

#### 1.1 Introduction to Matrix Algebra

What is Matrix?

-   A way to group number or symbols together, where you have rows and columns
-   A matrix is a rectangular array of elements arranged in rows and columns

The dimension of a matrix is giving by:

-   Rows
-   Columns
-   Rows X Columns $r x c$

Here we have a 3x2 Matrix

```{r}
matrix(data = c(1, 2, 3, 4, 5, 6), 
       nrow = 3, 
       ncol = 2)
```

Rather than using numbers, we can represent parts of a matrix (and the matrix itself) by symbols.

Example:


$$A = \begin{bmatrix} a_{11} \ a_{12} \ a_{13} \\ a_{21} \ a_{22} \ a_{23} \end{bmatrix}$$

Where $a_{ij}$ is the row $i$ and column $j$ element of $A$. $a_{11}$ is often called $(1,1)$ element of $A$, $a_{12}$ is the $(1,2)$ element of \$A.

When writing in hand we cannot bold the letter $A$ so we use the underlined $\underline{A}$

Square matrix is $r x c$ where $r = c$

**Vector** - A $r x 1$ is a *column vector*

**Vector** - A $1 x r$ is a *row vector*

Both being a special case matrix.

```{r}
matrix(data = c(1, 2, 3),
       ncol = 1,
       nrow = 3)
```

**Transpose** - Is when we interchange the rows and columns of a matrix or vector

Example:

$$A = \begin{bmatrix} a_{11} \ a_{12} \ a_{13} \\ a_{21} \ a_{22} \ a_{23} \end{bmatrix}$$

$$A' = \begin{bmatrix} a_{11} \ a_{21} \\ a_{12} \ a_{22} \\ a_{13} \ a_{23} \end{bmatrix}$$


```{r}
matrix(data = c(1, 2, 3, 4, 5, 6),
       nrow = 2,
       ncol = 3)
```

```{r}
matrix(data = c(1, 2, 3, 4, 5, 6),
       nrow = 3,
       ncol = 2)
```

$A$ is 2x3 and $A'$ is a 3x2. The $'$ symbol indicates a transpose, and it is said as the word *prime*. Thus, the transpose of $A$ is *A prime*.

##### 1.1.1 Introduction to Matrix Algebra - Matrix addition and subtraction

Adding or subtraction the corresponding elements of matrices *with the same dimension*.

Example

```{r}
MA <- matrix(data = c(1, 2, 3, 5, 6, 7), 
             nrow = 2,
             ncol = 3,
             byrow = TRUE)

MB <- matrix(data = c(-1, 10, -1, 5, 5, 8),
             nrow = 2,
             ncol = 3,
             byrow = TRUE)
MA
MB
```

Then $A + B$ is:

```{r}
MA + MB
```

and $A - B$ is:

```{r}
MA - MB
```

`byrow` by default is set to `FALSE`. Thus, the number would be entered into the matrix by columns. Example

```{r}
MX <- matrix(data = c(1, 2, 3, 5, 6, 7), 
             nrow = 2,
             ncol = 3,
             byrow = FALSE)
MX
class(MX)
```

A transpose of a matri can be done using the `t()` function.

```{r}
t(MX)
```

##### 1.1.2 Matrix multiplication

**Scalar** - Is a 1x1 matrix

Multiplied a matrix by a scalar

$$cA = \begin{bmatrix} ca_{11} \ ca_{22} \ ca_{13} \\ ca_{21} \ ca_{22} \ ca_{23} \end{bmatrix}$$

Where $c$ is the scalar. When $A$ is equal to

```{r}
MA <- matrix(data = 1:6, 
             nrow = 2, 
             ncol = 3,
             byrow = TRUE)
```

and $c = 2$, then $2A$ is:

```{r}
MA*2
```

**Multiplying two matrices**

If we want to multiply $A$ for $B$ $A*B$ we first need the number of columns of $A$ to be the same as the number of rows from $B$. So $A_{rows} = B_{columns}$.

Example

$A$ is a 2x3 and $B$ is a $3x10$, then you can multiply this matrices. However, if $B$ is a 4x10 instead, these matrices cannot be multiplied.

The resulting dimentions of $C = A*B$:

- The number of rows of $A$ is the number of rows of $C$.
- The number of columns of $B$ is the number of rows of $C$
- In a 2x3 * 3x10 the result matrix will be 2x10

$$C = AB = \begin{bmatrix} 1 \ 2 \ 3 \\ 4 \ 5 \ 6 \end{bmatrix} * \begin{bmatrix} 3 \ 0 \\ 1 \ 2 \ \\ 0 \ 1 \end{bmatrix}$$

$$= \begin{bmatrix} 1*3 + 2*1 + 3*0 \ 1*0 + 2*2 + 3*1 \\ 4*3 +5*1 +6*0 \ 4*3 + 5*1 + 6*0 \end{bmatrix} = \begin{bmatrix} 5 \ 7 \\ 17 \ 16 \end{bmatrix}$$

```{r}
MA <- matrix(data = 1:6,
             nrow = 2,
             ncol = 3,
             byrow = TRUE)

MB <- matrix(data = c(3, 0, 1, 2, 0, 1),
             ncol = 2,
             nrow = 3,
             byrow = TRUE)

MA
MB
```

```{r}
MC_1 = MA%*%MB
MC_1
```

The *cross product* of the rows of $A$ and the columns of $B$ are taken to form $C$.

Being $MA*MC \neq MB*MC$

```{r}
MC_2 = MB%*%MA
MC_2
```

The notation `%*%` is used to multiplied matrices and vectors, where `*` stays to perform elementwise multiplications, the $(i,j)$ elements of each matrix are multiplied together.

Multiplying vectors with other vector or matrices in R can be confusing because no row or column dimensions are given for a vector object. Example

```{r}
MX <- c(1, 2, 3)
```

```{r}
MX%*%MX
MA%*%MX
```

How does R know that we want $MX'MX$ instead of $MXMX'$ when we have not told R that $MX$ is a 3x1?

Similarly, how does R know that $MA*MX$ is a 2x1. From `R Help` `%*%`

*Multiplies two matrices, if they are conformable. If one argument is a vector, it will be promoted to either a row or column matrix to make the two arguments conformable. If both are vectors it will return the inner product.*

An inner product produces a scalar value. If you wanted a $MXMX'$, one can use the outer product `%o%`

```{r}
MX%o%MX
```

Exercices - Using the `GPA` dataset to see how High School score could be use as an independent variable `X` to predict College Scores `Y`.

- Find $X'$, $X'X$, $X'Y$ and $Y'Y$

```{r}
gpa <- read.csv2(url("https://raw.githubusercontent.com/PedroTL/bookdown-study/main/introduction-multivariated-analysis-r/Banco%20de%20Dados/gpa.csv"), sep = ",") |>
  dplyr::mutate_if(is.character, as.numeric)
```

Where `mpg` is $X$ and `drat` is $Y$ 

```{r}
X <- cbind(1, gpa$HSGPA)
head(X)
```

```{r}
Y <- gpa$CollegeGPA
head(Y)
```

Finding $X'$

```{r}
t(X)
```

Finding $X'X$

![](C:\Users\pedro\Documents\GitHub\bookdown-study\introduction-multivariated-analysis-r\images\matrix_5.png)

```{r}
t(X) %*% X
```

Finding $X'Y$

![](C:\Users\pedro\Documents\GitHub\bookdown-study\introduction-multivariated-analysis-r\images\matrix_6.png)

```{r}
t(X) %*% Y
```

Finding $Y'Y$

![](C:\Users\pedro\Documents\GitHub\bookdown-study\introduction-multivariated-analysis-r\images\matrix_7.png)

```{r}
t(Y) %*% Y
```

- `cbind()` combines items by *columns*
- `rbind()` combines items by *rows*

**Special types of matrices**

Symmetric matrix - If $A$ = $A'$, then $A$ is symmetric

```{r}
MA <- matrix(data = c(1, 2, 2, 3),
             nrow = 2, 
             ncol = 2,
             byrow = TRUE)
MA

t(MA)
```

Diagonal Matrix - A square matrix whose *off-diagonal* elements are 0. 

- The diagonal of a matrix are the $(1,1)$, $(2,2)$, $(3,3)$ elements.

```{r}
MA <- matrix(data = c(1, 0, 0, 0, 1, 0, 0, 0, 1),
             nrow = 3,
             ncol = 3,
             byrow = TRUE)
MA
```


Identity Matrix - A diagonal matrix whose *diagonals* elements are 1. 

- The diagonal of a matrix are the $(1,1)$, $(2,2)$, $(3,3)$ elements.

```{r}
MI <- matrix(data = c(1, 1, 2, 3, 1, 0, 0, 0, 1),
             nrow = 3,
             ncol = 3,
             byrow = TRUE)
MI
```

Note that $I$ (The letter, not the number) usually denotes the identity matrix.

If we multiply a matrix $A$ of same dimentions of a matrix $I$ we get as a result matrix $A$, thats why is *identity*

Vectors of 0's - Vector that only contains zeros

**Linear dependence and rank of matrix**

Let $A$ think of each column of $A$ as a vector; i.e., $A = [a_1 a_2, a_3]$. Note that $3a_2 = a_3$. This means the columns of $A$ are *linearly dependent*.

Formally, a set of column vectors are linearly dependent if there exists constants $\lambda_1, \lambda_2, ..., \lambda_c$ (not all zero), such that $\lambda_1a_1 + \lambda_2a_2 + ... + \lambda_ca_c = 0$. A set of column vectors are linearly independent if $\lambda_1a_1 + \lambda_2a_2 + ... + \lambda_ca_c = 0$, only for $\lambda_1 = \lambda_2 = ... = \lambda_c = 0$.

The rank of matrix in the maximum number of linearly independent columns in the matrix.

$rank(A) = 2$

If a matrix does not have a rank equal to the number of its columns, this can cause problems in some of our calculations.

```{r}
MA <- matrix(data = c(1, 2, 6, 3, 4, 12, 5, 6, 18),
             nrow = 3, 
             ncol = 3,
             byrow = TRUE)
MA
```

**Inverse of a Matrix**

The inverse of a scalar, say $b$, is $b^{-1}$. For example, the inverse of $b = 3$ is $3^{-1} = {1/3}$. Also, $b*b^{-1} = 1$.

In matrix algebra, the inverse of a matrix is another matrix.

For example, the inverse of $A$ is $A^{-1}$, and $AA^{-1} = A{^-1}A = I$. Note that $A must be a square matrix.

$A$

```{r}
MA <- matrix(data = 1:4, 
             nrow = 2, 
             ncol = 2,
             byrow = TRUE)
MA
```

$A^{-1}$

```{r}
MA_up_to_minus_one <- matrix(data = c(-2, 1, 1.5, -0.5),
                             ncol = 2,
                             nrow = 2,
                             byrow =TRUE)
MA_up_to_minus_one
```

```{r}
MA %*% MA_up_to_minus_one
```


![](C:\Users\pedro\Documents\GitHub\bookdown-study\introduction-multivariated-analysis-r\images\matrix_8.png)

**Finding the inverse**

For a 2x2 matrix, there is a simple formula.

![](C:\Users\pedro\Documents\GitHub\bookdown-study\introduction-multivariated-analysis-r\images\matrix_9.png)

For larger matrix we rely on R.

Exercices

```{r}
MA <- matrix(data = 1:4,
             nrow = 2,
             ncol = 2,
             byrow = TRUE)
solve(MA)
```

```{r}
MA %*% solve(MA)
```

```{r}
solve(MA) %*% MA
```

```{r}
round(solve(MA) %*% MA, 2)
```

In R the `solve()` inverts a matrix. The `solve(MA, Mb)`, function can also be used to *solve* for $x$ in $Ax = b$ since $A^{-1}Ax = A^{-1}b \rightarrow Ix = A^{-1}b \rightarrow A^{-1}b$

Example with `gpa` dataset

- Find $(X'X)^{-1}$
- Find $(X'X)^{-1}X'Y$

```{r}
solve(t(X) %*% X)
```

```{r}
solve(t(X) %*% X) %*% t(X) %*% Y
```

Thus, $$(X'X)^{-1}X'Y = \begin{bmatrix} \hat{\beta}_0 \\ \hat{\beta}_1 \end{bmatrix} $$, where $\hat{\beta}_0$ and $\hat{\beta}_1$ are the estimated intercept and slope coefficients in a simple linear regression model.

We can confirm by making a fitted model.

```{r}
mod.fit <- lm(CollegeGPA ~ HSGPA, data = gpa)
mod.fit$coefficients
```

**Trace**

Suppose a square matrix $A$

Trace - The trace of a square matrix $A$ is defined as $tr(A) = \overset{p}{\underset{i=1}{\sum}} a_{ij} = a_{11} + a_{22} + ... a_{pp}$; i.e., the sum of a square matrix diagonal elements. 

```{r}
MA <- matrix(data = 1:4,
             nrow = 2, 
             ncol =2,
             byrow = TRUE)
MA
```

```{r}
diag(MA)
```

```{r}
sum(diag(MA))
```

The covariance `cov()` is simillar to the correlation `corr()`. What changes is that the `cov()` is not bounded to -1 and 1. 

$$corr(x_1, x_2) = \frac{cov(x_1, x_2) }{(\sqrt(var(x_1) * \sqrt(var(x_2))}$$
Taking the sum of the diagonal of a covariance matrix would tell all the variability of the data, important for the PCA analysis. 

**Determinant**

The determinant of a square matrix $A$ is $|A| = \overset{p}{\underset{i=1}{\sum}} a_{ij}A_{ij} where A_{1j} = (-1)^{1+j}|A^{1j}|$ and $A^{1j}$ is obtained from $A$ by deleting its first row and its $j^{th}$ columns.

The determinant for a 2x2 matrix is defined as

$$\begin{bmatrix} a_{11} \ a_{12} \\ a_{21} \ a_{22} \end{bmatrix} = a_{11}a_{22} - a_{12}a_{21}$$

That is, the product of the diagonal elements $a_{11}, a_{22}$ minus the product of the off diagonal elements $a_{12}, a_{21}$

The determinant for a 3x3 matrix can be defined as

$$\begin{bmatrix} a_{11} \ a_{12} \ a_{13} \\ a_{21} \ a_{22} \ a_{23} \\ a_{31} \ a_{32} \ a_{33} \end{bmatrix} = a_{11} \begin{bmatrix} a_{22} \ a_{23} \\ a_{32} \ a_{33} \end{bmatrix} - a_{12}  \begin{bmatrix} a_{21} \ a_{23} \\ a_{31} \ a_{33}  \end{bmatrix} + a_{13} \begin{bmatrix} a_{21} \ a_{22} \\ a_{31} \ a_{32} \end{bmatrix}$$
In 3x3 matrix we form 2x2 matrices with elements of the 3x3 matrix. For instance, the $$\begin{bmatrix} a_{22} \ a_{23} \\ a_{32} \ a_{33} \end{bmatrix}$$

is an extracted of the 3x3 matrix and is being multiplied by the first element of the original 3x3 matrix $a_{11}$. This multiplications happens with the element that is not in the same row or column of the extracted matrix.

Fallowing, the $$\begin{bmatrix} a_{21} \ a_{23} \\ a_{31} \ a_{33}  \end{bmatrix}$$

is and extracted of the 3x3 matrix and fallow the same pattern, being multiplied by the $a_{12}$ element, that is not in the same row or column of the extracted matrix from the 3x3 matrix

```{r}
MA <- matrix(data = c(1, 2, 3, 4),
             nrow = 2,
             ncol = 2,
             byrow = TRUE)
MA
```

```{r}
det(MA)
```

**Eingenvalues and eigenvectors**

Suppose we have a pxp matrix $A$, being square matrix.

$$A = \begin{bmatrix} a_{11} \ a_{12} \ ... \ a_{1p} \\ a_{21} \ a_{22} \ ... \ a_{2p} \\ ... \\ a_{p1} \ a_{p2} \ a_{pp} \end{bmatrix}$$

Eingenvalues - Are the roots of the polynomial equation defined by $|A - \lambdaI| = 0$ where $I$ is an identity matrix and $\lambda$ is a constant. We then find the determinant of that, set all to zero and solved for $\lambda$. The $\lambda$ we find in the end is the *Eingenvalues*

if $p = 2$, then the eigenvalues are the roots of

$$\begin{bmatrix} a_{11} \ a_{12} \\ a_{21} \ a_{22} \end{bmatrix} - \begin{bmatrix} \lambda \ 0 \\ 0 \ \lambda \end{bmatrix} = \begin{bmatrix} a_{11} - \lambda \ a_{12} \\ a_{21} \ a_{22} - \lambda \end{bmatrix}$$

$$= (a_{11} - \lambda) (a_{22} - \lambda) - a_{21}a_{12}$$

$$= \lambda^2 - \lambda(a_{11} + a_{22}) - (a_{21}a_{12} - a_{11}a_{22}) = 0$$

Using the quadratic formula, we obtain

$$\lambda = \frac {a_{11} + a_{22} \pm \sqrt{(a_{11} + a_{22})^2 + 4(a_{21}a_{12} - a_{11}a_{22})}} 2 $$

In general, the eigenvalues are the p roots of $c_{1}\lambda^{p} + c_{2}\lambda^{p-1} + c_{3}\lambda^{p-2} + ... + c_{p}\lambda + c_{p+1} = 0$ where $c_i$ for $i = 1, ..., p+1$ denote constants.

When $A$ is symmetric matrix, the eigenvalues are real numbers and can be ordered from largest to smallest as $\lambda_1 \ge \lambda_2 \ge ... \ge \lambda_p$ where $\lambda_1$ is the largest.

```{r}
MA <- matrix(data = c(1, 0.5, 0.5, 1.25),
             nrow = 2, 
             ncol = 2,
             byrow = TRUE)
MA
```

```{r}
save_eigen <- eigen(MA)
save_eigen
```

```{r}
save_eigen$values
save_eigen$vectors
```

$$\lambda = \frac {2.25 \pm \sqrt{(2.25^2 + 4(0.25 - 1.25))}} 2$$ 

This is equal to $1.6404$ and $0.6096$

Eigenvectors - Each eigenvalue of $A$ has a corresponding nonzero vector $b$ called an eigenvector that satisfies $Ab = \lambda b$. If i take $Ab$ where $b$ is an eigenvector this will be equal to $\lambda b$. So, values of $b$ that satisfied this relationship are called eigenvectors.

Eigenvectors for a particular eigenvalue are not unique. They are also given as having a length of 1. When two eigenvalues are not equal, their corresponding eigenvecctors are orthogonal (i.e., $b_{i}'b_j = 0$).

If i would take the trace of matrix $A$ that is the sum of the eigenvalues $tr(A) = \sum{\lambda_i}$

